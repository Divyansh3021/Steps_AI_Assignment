{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class NvidiaDocsSpider:\n",
    "    def __init__(self):\n",
    "        self.allowed_domains = [\"docs.nvidia.com\"]\n",
    "        self.start_urls = [\"https://docs.nvidia.com/cuda/\"]\n",
    "        self.visited_urls = set()\n",
    "        self.max_depth = 5\n",
    "\n",
    "    def parse(self, url, depth):\n",
    "        if url in self.visited_urls or depth > self.max_depth:\n",
    "            return []\n",
    "        self.visited_urls.add(url)\n",
    "        print(f\"Scraping: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_content = soup.get_text()\n",
    "\n",
    "        data = [{\n",
    "            'url': url,\n",
    "            'content': page_content\n",
    "        }]\n",
    "\n",
    "        # Find and follow sub-links\n",
    "        if depth < self.max_depth:\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(url, link['href'])\n",
    "                if self.is_allowed_domain(next_url):\n",
    "                    data.extend(self.parse(next_url, depth + 1))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def is_allowed_domain(self, url):\n",
    "        return any(domain in url for domain in self.allowed_domains)\n",
    "\n",
    "    def run(self):\n",
    "        data = []\n",
    "        for url in self.start_urls:\n",
    "            data.extend(self.parse(url, 0))\n",
    "        with open('nvidia_docs.json', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "scraper = NvidiaDocsSpider()\n",
    "scraper.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Google Generative AI Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    google_api_key=os.environ['GOOGLE_API_KEY'],\n",
    "    task_type=\"retrieval_document\"\n",
    ")\n",
    "\n",
    "# Load the scraped data\n",
    "with open('nvidia_docs.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize Semantic Chunker\n",
    "splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Chunk data\n",
    "chunks = []\n",
    "for entry in data:\n",
    "    if entry['content'].strip():  # Ensure the content is not empty\n",
    "        docs = splitter.create_documents([entry['content']])\n",
    "        for doc in docs:\n",
    "            content = doc.page_content.strip()\n",
    "            if content:  # Ensure each chunk is not empty\n",
    "                chunks.append({\n",
    "                    'url': entry['url'],\n",
    "                    'content': content,\n",
    "                    'embedding': embeddings.embed_query(content)\n",
    "                })\n",
    "\n",
    "# Save the chunks\n",
    "with open('nvidia_chunks.json', 'w') as f:\n",
    "    json.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milvus import default_server\n",
    "default_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "connections.connect(\n",
    "   host='127.0.0.1',\n",
    "   port=default_server.listen_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=500, is_primary=True),\n",
    "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length =65535),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768)\n",
    "]\n",
    "collection_schema = CollectionSchema(fields=fields, schema=\"DenseVector\")\n",
    "collection_name_ivf = \"ivf_embeddings\"\n",
    "\n",
    "# Define IVF parameters\n",
    "nlist = 128\n",
    "metric = \"L2\" \n",
    "\n",
    "collection = Collection(name=collection_name_ivf, schema=collection_schema, use_index=\"IVF_FLAT\", params={\"nlist\": nlist, \"metric\": metric})\n",
    "\n",
    "entity = []\n",
    "for chunk in chunks:\n",
    "    dic = {}\n",
    "    dic['url'] = chunk['url']\n",
    "    dic['content'] = chunk['content']\n",
    "    dic['embedding'] = chunk['embedding']\n",
    "    entity.append(dic)\n",
    "\n",
    "collection.insert(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "llm = genai.GenerativeModel('models/gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion(query):\n",
    "    prompt = f\"\"\"\n",
    "    System: You are a helpful expert technical research assistant. Provide an example answer to the given question, that might be found in a document like an web scraped data. \n",
    "    \n",
    "    User: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    return llm.generate_content(prompt).text\n",
    "\n",
    "modified_query = query_expansion(\"How to install CUDA on Linux?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Installing CUDA on Linux - Example using Ubuntu 20.04 and CUDA 11.4\\n\\nWhile specifics vary depending on your Linux distribution and chosen CUDA version, here's a common installation method:\\n\\n**1. Verify GPU Compatibility:**\\n\\n    * Run `lspci | grep -i nvidia` to confirm your GPU is NVIDIA and supported. \\n    * Cross-reference your GPU model with the CUDA Toolkit documentation for compatibility.\\n\\n**2. Download CUDA Toolkit:**\\n\\n    * Visit the NVIDIA Developer website: [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)\\n    * Select your Linux distribution, version (e.g., Ubuntu 20.04), architecture (x86_64), installer type (runfile recommended), and click Download.\\n\\n**3. Install CUDA Toolkit:**\\n\\n    * Open a terminal.\\n    * Navigate to the download directory: `cd Downloads`\\n    * Make the file executable: `chmod +x cuda_11.4.1_470.57.05_linux.run`\\n    * Run the installer: `sudo ./cuda_11.4.1_470.57.05_linux.run`\\n    * Follow the on-screen prompts. Generally, accepting defaults is sufficient. \\n    * **IMPORTANT:** Deselect the driver installation if you already have a compatible NVIDIA driver installed.\\n\\n**4. Set Environment Variables:**\\n\\n    * Add CUDA paths to your `.bashrc` file:\\n       ```bash\\n       echo 'export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}' >> ~/.bashrc\\n       echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc\\n       source ~/.bashrc \\n       ```\\n\\n**5. Verify Installation:**\\n\\n    * Compile and run the provided CUDA samples:\\n       ```bash\\n       cd /usr/local/cuda-11.4/samples/1_Utilities/deviceQuery\\n       sudo make\\n       ./deviceQuery\\n       ```\\n    * You should see information about your GPU if the installation was successful.\\n\\n**Important Notes:**\\n\\n* Replace version numbers (CUDA 11.4) with your specific choices.\\n* Installing the wrong driver can cause system instability.  \\n* Reboot your system after installation.\\n* Consult the official NVIDIA CUDA installation guide for detailed instructions and troubleshooting: [https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/) \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Runfile Installationï\\x83\\x81\\n\\nBasic instructions can be found in the Quick Start Guide. Read on for more detailed instructions. This section describes the installation and configuration of CUDA when using the standalone installer. The standalone installer is a â\\x80\\x9c.runâ\\x80\\x9d file and is completely self-contained. 8.1. Runfile Overviewï\\x83\\x81\\n\\nThe Runfile installation installs the NVIDIA Driver and CUDA Toolkit via an interactive ncurses-based interface. The installation steps are listed below. Distribution-specific instructions on disabling the Nouveau drivers as well as steps for verifying device node creation are also provided. Finally, advanced options for the installer and uninstallation steps are detailed below. The Runfile installation does not include support for cross-platform development. For cross-platform development, see the CUDA Cross-Platform Environment section. 8.2. Installationï\\x83\\x81\\n\\n\\nPerform the pre-installation actions. Disable the Nouveau drivers. Reboot into text mode (runlevel 3). This can usually be accomplished by adding the number â\\x80\\x9c3â\\x80\\x9d to the end of the systemâ\\x80\\x99s kernel boot parameters. Since the NVIDIA drivers are not yet installed, the text terminals may not display correctly. Temporarily adding â\\x80\\x9cnomodesetâ\\x80\\x9d to the systemâ\\x80\\x99s kernel boot parameters may fix this issue. Consult your systemâ\\x80\\x99s bootloader documentation for information on how to make the above boot parameter changes. The reboot is required to completely unload the Nouveau drivers and prevent the graphical interface from loading. The CUDA driver cannot be installed while the Nouveau drivers are loaded or while the graphical interface is active. Verify that the Nouveau drivers are not loaded. If the Nouveau drivers are still loaded, consult your distributionâ\\x80\\x99s documentation to see if further steps are needed to disable Nouveau. Run the installer and follow the on-screen prompts:\\n\\nsudo sh cuda_<version>_linux.run\\n\\n\\nThe installer will prompt for the following:\\n\\nEULA Acceptance\\nCUDA Driver installation\\nCUDA Toolkit installation, location, and /usr/local/cuda symbolic link\\n\\nThe default installation location for the toolkit is /usr/local/cuda-12.4:\\nThe /usr/local/cuda symbolic link points to the location where the CUDA Toolkit was installed. This link allows projects to use the latest CUDA Toolkit without any configuration file update. The installer must be executed with sufficient privileges to perform some actions. When the current privileges are insufficient to perform an action, the installer will ask for the userâ\\x80\\x99s password to attempt to install with root privileges. Actions that cause the installer to attempt to install with root privileges are:\\n\\ninstalling the CUDA Driver\\ninstalling the CUDA Toolkit to a location the user does not have permission to write to\\ncreating the /usr/local/cuda symbolic link\\n\\nRunning the installer with sudo, as shown above, will give permission to install to directories that require root permissions. Directories and files created while running the installer with sudo will have root ownership. If installing the driver, the installer will also ask if the openGL libraries should be installed. If the GPU used for display is not an NVIDIA GPU, the NVIDIA openGL libraries should not be installed. Otherwise, the openGL libraries used by the graphics driver of the non-NVIDIA GPU will be overwritten and the GUI will not work. If performing a silent installation, the --no-opengl-libs option should be used to prevent the openGL libraries from being installed. See the Advanced Options section for more details. If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf, may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information. Note\\nInstalling Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries. Reboot the system to reload the graphical interface:\\n\\nsudo reboot\\n\\n\\n\\nVerify the device nodes are created properly. Perform the post-installation actions. 8.3. Disabling Nouveauï\\x83\\x81\\n\\nTo install the Display Driver, the Nouveau drivers must first be disabled. Each distribution of Linux has a different method for disabling Nouveau. The Nouveau drivers are loaded if the following command prints anything:\\n\\nlsmod | grep nouveau\\n\\n\\n\\n\\n8.3.1. Fedoraï\\x83\\x81\\n\\n\\n\\nCreate a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:\\n\\nblacklist nouveau\\noptions nouveau modeset=0\\n\\n\\n\\n\\nRegenerate the kernel initramfs:\\n\\nsudo dracut --force\\n\\n\\n\\n\\nRun the following command:\\n\\nsudo grub2-mkconfig -o /boot/grub2/grub.cfg\\n\\n\\n\\nReboot the system. 8.3.2. RHEL / Rocky and KylinOSï\\x83\\x81\\n\\n\\n\\nCreate a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:\\n\\nblacklist nouveau\\noptions nouveau modeset=0\\n\\n\\n\\n\\nRegenerate the kernel initramfs:\\n\\nsudo dracut --force\\n\\n\\n\\n\\n\\n\\n\\n8.3.3. OpenSUSEï\\x83\\x81\\n\\n\\n\\nCreate a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:\\n\\nblacklist nouveau\\noptions nouveau modeset=0\\n\\n\\n\\n\\nRegenerate the kernel initrd:\\n\\nsudo /sbin/mkinitrd\\n\\n\\n\\n\\n\\n\\n\\n8.3.4. SLESï\\x83\\x81\\n\\nNo actions to disable Nouveau are required as Nouveau is not installed on SLES. 8.3.5. WSLï\\x83\\x81\\n\\nNo actions to disable Nouveau are required as Nouveau is not installed on WSL. 8.3.6. Ubuntuï\\x83\\x81\\n\\n\\n\\nCreate a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:\\n\\nblacklist nouveau\\noptions nouveau modeset=0\\n\\n\\n\\n\\nRegenerate the kernel initramfs:\\n\\nsudo update-initramfs -u\\n\\n\\n\\n\\n\\n\\n\\n8.3.7. Debianï\\x83\\x81\\n\\n\\n\\nCreate a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:\\n\\nblacklist nouveau\\noptions nouveau modeset=0\\n\\n\\n\\n\\nRegenerate the kernel initramfs:\\n\\nsudo update-initramfs -u\\n\\n\\n\\n\\n\\n\\n\\n\\n8.4. Device Node Verificationï\\x83\\x81\\n\\nCheck that the device files/dev/nvidia* exist and have the correct (0666) file permissions. These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver. Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuidnvidia-modprobe tool that is bundled with the NVIDIA Driver. However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below:\\n\\n#!/bin/bash\\n\\n/sbin/modprobe nvidia\\n\\nif [ \"$?\" -eq 0 ]; then\\n  # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA`\\n  N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l`\\n  NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l`\\n\\n  N=`expr $N3D + $NVGA - 1`\\n  for i in `seq 0 $N`; do\\n    mknod -m 666 /dev/nvidia$i c 195 $i\\n  done\\n\\n  mknod -m 666 /dev/nvidiactl c 195 255\\n\\nelse\\n  exit 1\\nfi\\n\\n/sbin/modprobe nvidia-uvm\\n\\nif [ \"$?\" -eq 0 ]; then\\n  # Find out the major device number used by the nvidia-uvm driver\\n  D=`grep nvidia-uvm /proc/devices | awk \\'{print $1}\\'`\\n\\n  mknod -m 666 /dev/nvidia-uvm c $D 0\\nelse\\n  exit 1\\nfi\\n\\n\\n\\n\\n\\n8.5. Advanced Optionsï\\x83\\x81\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAction\\nOptions Used\\nExplanation\\n\\n\\n\\n\\nSilent Installation\\n--silent\\nRequired for any silent installation.', 153.6268426911561), ('1. Introduction — Quick Start Guide 12.5 documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1. Introduction\\n2. Windows\\n2.1. Network Installer\\n2.2. Local Installer\\n2.3. Pip Wheels - Windows\\n2.4. Conda\\n\\n\\n3. Linux\\n3.1. Linux x86_64\\n3.1.1. Redhat / CentOS\\n3.1.1.1. RPM Installer\\n3.1.1.2. Runfile Installer\\n\\n\\n3.1.2. Fedora\\n3.1.2.1. RPM Installer\\n3.1.2.2. Runfile Installer\\n\\n\\n3.1.3. SUSE Linux Enterprise Server\\n3.1.3.1. RPM Installer\\n3.1.3.2. Runfile Installer\\n\\n\\n3.1.4. OpenSUSE\\n3.1.4.1. RPM Installer\\n3.1.4.2. Runfile Installer\\n\\n\\n3.1.5. Amazon Linux 2023\\n3.1.5.1. Prepare Amazon Linux 2023\\n3.1.5.2. Local Repo Installation for Amazon Linux\\n3.1.5.3. Network Repo Installation for Amazon Linux\\n3.1.5.4. Common Installation Instructions for Amazon Linux\\n\\n\\n3.1.6. Pip Wheels - Linux\\n3.1.7. Conda\\n3.1.8. WSL\\n3.1.9. Ubuntu\\n3.1.9.1. Debian Installer\\n3.1.9.2. Runfile Installer\\n\\n\\n3.1.10. Debian\\n3.1.10.1. Debian Installer\\n3.1.10.2. Runfile Installer\\n\\n\\n\\n\\n\\n\\n4. Notices\\n4.1. Notice\\n4.2. OpenCL\\n4.3. Trademarks\\n\\n\\n\\n\\n\\n\\n\\n\\nQuick Start Guide\\n\\n\\n\\n\\n\\n »\\n1. Introduction\\n\\n\\n\\nv12.5 |\\nPDF\\n|\\nArchive\\n\\xa0\\n\\n\\n\\n\\n\\n\\nCUDA Quick Start Guide\\nMinimal first-steps instructions to get CUDA running on a standard system. 1. Introductionï\\x83\\x81\\nThis guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform. These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide. The CUDA installation packages can be found on the CUDA Downloads Page. 2. Windowsï\\x83\\x81\\nWhen installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide. 2.1. Network Installerï\\x83\\x81\\nPerform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to download and install all components. Once the download completes, the installation will begin automatically. Once the installation completes, click â\\x80\\x9cnextâ\\x80\\x9d to acknowledge the Nsight Visual Studio Edition installation summary. Click close to close the installer. Navigate to the Samplesâ\\x80\\x99 nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody. Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln. Open the Build menu within Visual Studio and click Build Solution. Navigate to the CUDA Samples build directory and run the nbody sample. Note\\nRun samples by navigating to the executableâ\\x80\\x99s location, otherwise it will fail to locate dependent resources. 2.2. Local Installerï\\x83\\x81\\nPerform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to install all components. Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary. Click close to close the installer. Navigate to the Samplesâ\\x80\\x99 nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody. Open the nbody Visual Studio solution file for the version of Visual Studio you have installed. Open the Build menu within Visual Studio and click Build Solution. Navigate to the CUDA Samples build directory and run the nbody sample. Note\\nRun samples by navigating to the executableâ\\x80\\x99s location, otherwise it will fail to locate dependent resources. 2.3. Pip Wheels - Windowsï\\x83\\x81\\nNVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites\\nTo install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. py -m pip install --upgrade setuptools pip wheel\\r\\n\\n\\nYou should now be able to install the nvidia-pyindex module. py -m pip install nvidia-pyindex\\r\\n\\n\\nIf your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:\\n--extra-index-url https://pypi.ngc.nvidia.com\\r\\n\\n\\nProcedure\\nInstall the CUDA runtime package:\\npy -m pip install nvidia-cuda-runtime-cu12\\r\\n\\n\\nOptionally, install additional packages as listed below using the following command:\\npy -m pip install nvidia-<library>\\r\\n\\n\\nMetapackages\\nThe following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. â\\x80\\x9ccu12â\\x80\\x9d should be read as â\\x80\\x9ccuda12â\\x80\\x9d. nvidia-cuda-runtime-cu12\\nnvidia-cuda-cupti-cu12\\nnvidia-cuda-nvcc-cu12\\nnvidia-nvml-dev-cu12\\nnvidia-cuda-nvrtc-cu12\\nnvidia-nvtx-cu12\\nnvidia-cuda-sanitizer-api-cu12\\nnvidia-cublas-cu12\\nnvidia-cufft-cu12\\nnvidia-curand-cu12\\nnvidia-cusolver-cu12\\nnvidia-cusparse-cu12\\nnvidia-npp-cu12\\nnvidia-nvjpeg-cu12\\n\\nThese metapackages install the following packages:\\n\\nnvidia-nvml-dev-cu125\\nnvidia-cuda-nvcc-cu125\\nnvidia-cuda-runtime-cu125\\nnvidia-cuda-cupti-cu125\\nnvidia-cublas-cu125\\nnvidia-cuda-sanitizer-api-cu125\\nnvidia-nvtx-cu125\\nnvidia-cuda-nvrtc-cu125\\nnvidia-npp-cu125\\nnvidia-cusparse-cu125\\nnvidia-cusolver-cu125\\nnvidia-curand-cu125\\nnvidia-cufft-cu125\\nnvidia-nvjpeg-cu125\\n\\n\\n\\n2.4. Condaï\\x83\\x81\\nThe Conda packages are available at https://anaconda.org/nvidia. Installation\\nTo perform a basic install of all CUDA Toolkit components using Conda, run the following command:\\nconda install cuda -c nvidia\\r\\n\\n\\nUninstallation\\nTo uninstall the CUDA Toolkit using Conda, run the following command:\\nconda remove cuda\\r\\n\\n\\n\\n\\n\\n3. Linuxï\\x83\\x81\\nCUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on. 3.1. Linux x86_64ï\\x83\\x81\\nFor development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.', 149.70196903232366), ('2.3. Verify the System Has gcc Installedï\\x83\\x81\\n\\nThe gcc compiler is required for development using the CUDA Toolkit. It is not required for running CUDA applications. It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly. To verify the version of gcc installed on your system, type the following on the command line:\\n\\ngcc --version\\n\\n\\nIf an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web. 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installedï\\x83\\x81\\n\\nThe CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. For example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed. While the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed. However, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using. Therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version. The version of the kernel your system is running can be found by running the following command:\\n\\nuname -r\\n\\n\\nThis is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers. This command will be used multiple times below to specify the version of the packages to install. Note that below are the common-case scenarios for kernel usage. More advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running. Note\\nIf you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed. Otherwise, the CUDA Driver will fail to work with the new kernel. 2.5. Install GPUDirect Storageï\\x83\\x81\\n\\nIf you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package. GDS packages can be installed using the CUDA packaging guide. Follow the instructions in MLNX_OFED Requirements and Installation. GDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode. Installation instructions for them differ slightly. Compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations. Full GDS support is restricted to the following Linux distros:\\n\\nUbuntu 20.04, Ubuntu 22.04\\nRHEL 8.3, RHEL 8.4, RHEL 9.0\\n\\nStarting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver. Follow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages.', 148.24921918467564)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load passages and embeddings from JSON\n",
    "with open('nvidia_chunks.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract passages and embeddings into separate lists\n",
    "passages = [entry['content'] for entry in data]\n",
    "embeddings = torch.tensor([entry['embedding'] for entry in data])\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi([passage.split() for passage in passages])\n",
    "\n",
    "# Load DPR models and tokenizers\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "def encode_query(query):\n",
    "    inputs = context_tokenizer(query, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = context_encoder(**inputs).pooler_output\n",
    "    return query_embedding\n",
    "\n",
    "def retrieve_passages_dpr(query_embedding, embeddings, passages, top_k=3):\n",
    "    similarities = torch.matmul(query_embedding, embeddings.T).squeeze(0)\n",
    "    top_k_indices = torch.topk(similarities, k=top_k).indices\n",
    "    return [(passages[idx], similarities[idx].item()) for idx in top_k_indices]\n",
    "\n",
    "def retrieve_passages_bm25(query, passages, top_k=3):\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "    top_k_indices = torch.topk(torch.tensor(bm25_scores), k=top_k).indices\n",
    "    return [(passages[idx], bm25_scores[idx]) for idx in top_k_indices]\n",
    "\n",
    "def hybrid_retriever(query, embeddings, passages, top_k=3, alpha=0.5):\n",
    "    query_embedding = encode_query(query)\n",
    "    \n",
    "    assert query_embedding.shape[1] == embeddings.shape[1], f\"Query embedding size {query_embedding.shape} does not match passage embedding size {embeddings.shape}\"\n",
    "    \n",
    "    dpr_results = retrieve_passages_dpr(query_embedding, embeddings, passages, top_k)\n",
    "    bm25_results = retrieve_passages_bm25(query, passages, top_k)\n",
    "\n",
    "    # Combine DPR and BM25 results\n",
    "    combined_scores = {}\n",
    "    for passage, score in dpr_results:\n",
    "        combined_scores[passage] = combined_scores.get(passage, 0) + alpha * score\n",
    "    for passage, score in bm25_results:\n",
    "        combined_scores[passage] = combined_scores.get(passage, 0) + (1 - alpha) * score\n",
    "\n",
    "    sorted_passages = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_passages[:top_k]\n",
    "\n",
    "top_passages = hybrid_retriever(modified_query, embeddings, passages)\n",
    "print(top_passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context):\n",
    "    prompt = f\"\"\"\n",
    "    You are given this context and a query, based on the context and your own knowledge, answer the query.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    answer = llm.generate_content(prompt).text\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document provides a pretty thorough guide on installing CUDA on Linux. Here's a summarized breakdown based on the information:\n",
      "\n",
      "**Installation Methods**\n",
      "\n",
      "CUDA on Linux can be installed using various methods depending on your Linux distribution:\n",
      "\n",
      "* **RPM Package:** Suitable for distributions like Red Hat, CentOS, Fedora, SUSE Linux Enterprise Server, and OpenSUSE.\n",
      "* **Debian Package:** Ideal for Debian and Ubuntu-based systems.\n",
      "* **Runfile:** A self-contained installer that works across various distributions. It offers an interactive, ncurses-based installation process. \n",
      "* **Conda:** Available if you use the Conda package manager. \n",
      "\n",
      "**Common Steps (Especially Relevant for Runfile Installation)**\n",
      "\n",
      "1. **Prerequisites:**\n",
      "   * **GCC Compiler:** Ensure you have it installed as it's essential for CUDA development.\n",
      "   * **Kernel Headers and Development Packages:**  Match these to your running kernel version for driver compatibility. \n",
      "\n",
      "2. **Disable Nouveau Drivers:**\n",
      "   * Nouveau is an open-source driver that can conflict with the NVIDIA driver. Disabling it is crucial. \n",
      "   * The process varies by Linux distribution, but generally involves:\n",
      "      * Creating a blacklist file (e.g., `/etc/modprobe.d/blacklist-nouveau.conf`).\n",
      "      * Regenerating the kernel initramfs.\n",
      "\n",
      "3. **Reboot into Text Mode:**\n",
      "   * This step is often necessary to unload Nouveau fully and avoid conflicts.\n",
      "   * You'll likely need to edit kernel boot parameters (often by adding \"3\" and \"nomodeset\").\n",
      "\n",
      "4. **Run the Installer:**\n",
      "   * For Runfile: `sudo sh cuda_<version>_linux.run`\n",
      "   * Follow the on-screen prompts (EULA, driver installation, toolkit location, symbolic link setup).\n",
      "\n",
      "5. **Reboot to Reload GUI:** `sudo reboot`\n",
      "\n",
      "6. **Verify Installation:**\n",
      "   * Check for device nodes: `/dev/nvidia*` (should have correct permissions).\n",
      "   * Run a CUDA sample application (nbody is often recommended).\n",
      "\n",
      "**Additional Notes**\n",
      "\n",
      "* **GPUDirect Storage (GDS):** Requires specific CUDA and MLNX_OFED packages. \n",
      "* **Advanced Options:** The Runfile installer supports options like silent installation (`--silent`).\n",
      "* **Distribution-Specific Instructions:** Refer to the detailed instructions for your Linux distribution (Fedora, RHEL, OpenSUSE, SLES, Ubuntu, Debian) within the provided documentation.  The steps for disabling Nouveau and handling other distribution-specific aspects are explained there.\n",
      "\n",
      "**Where to Find More Information**\n",
      "\n",
      "The provided text seems to be part of the official CUDA documentation.  Look for:\n",
      "\n",
      "* **CUDA Quick Start Guide:** Provides a concise overview of installation steps.\n",
      "* **Windows Installation Guide and Linux Installation Guide:**  Go here for in-depth, platform-specific instructions. \n",
      "* **CUDA Downloads Page:** The source for the installation packages themselves. \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(\"How to install CUDA on linux?\", top_passages))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
